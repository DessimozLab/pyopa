\documentclass[12pt]{article}

\usepackage{listings} 
\usepackage{listings}
\usepackage{color}

\newcommand{\pp}{PyOPA}
\newcommand{\alenv}{\emph{AlignmentEnvironment}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
	language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

\title{Optimal Pairwise Alignment}

\author{}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Introduction}
The nascent field of bioinformatics is expanding at unprecedented rate, demanding more and more efficient implementations of a variety of algorithms, especially associated with sequence alignment.

In many cases hundreds of billions of alignments have to be done to investigate relations between publicly available genes. Such an immense amount of operations require an efficient low level - possibly vectorised - implementation, for which usually C/C++ is used. However, it is usually not really convenient to develop the applications that use the alignment algorithm in these low level languages.

\pp{} makes it possible to carry out various operations on sequences with the efficiency of a vectorised C and the convenience of a Python code, in a platform independent way. 

The C core, that is wrapped by using NumPy and Cython, is already broadly used from Darwin, so in order to make a platform-switching possible, all results must conform to the results coming from Darwin, which is assured by the unit tests coming with \pp{}.

In this document, we concisely describe the main functionalities offered by \pp{} as well as a brief comparison to PyCogent. In Section \ref{sec:inst} we show how to install \pp{}, while Section \ref{sec:features} outlines the main features of the package, followed by the comparison to PyCogent in Section \ref{sec:compCogent}. Finally, a short conclusion can be found in Section \ref{sec:concl}. 

\section{Installing \pp{}}
\label{sec:inst}
\pp{} draws on features offered by \emph{NumPy} and \emph{Cython} and it is strongly recommended to properly install the \emph{latest} version of these packages before installing \pp{}.

There are two simple ways to install \pp{}, the first, probably more convenient, is by using \emph{pip}:
\begin{lstlisting}
pip install PyOPA
\end{lstlisting}
This simple code snippet should automatically download \pp{} from \emph{PyPi} and compile and install the necessary files according to your operating system. This means, that on UNIX-based systems \emph{gcc}, and on Windows-based systems \emph{Visual Studio Compiler} must be available.

If you are able to install \emph{NumPy} and \emph{Cython} without any problems by using \emph{pip}, which means that you have the necessary means and permissions to download, compile and install a package from \emph{PyPi}, no errors should occur during the installation of \pp{}.

Another possibility is to download, extract and install the package manually. After extracting the project into a separate folder, you should navigate into the directory and use

\begin{lstlisting}
python setup.py install
\end{lstlisting}

to install \pp{}. This should compile and install all the necessary files without any problems. To check whether the unit tests produce correct results on your computer, you can simply run them by using the unittest framework:
\begin{lstlisting}
python -m unittest discover test/
\end{lstlisting}

Unless you have your own data files (where you define, for example, the distance matrix, gap opening and extension costs), you have to use one of the matrices coming with \pp{}. All the data files that is necessary to run the unit tests and future operations on sequences are automatically installed into a separate folder (\emph{\textless{}sys.prefix\textgreater/\pp{}\_test/}) during installation. If you cannot locate or access the data files, you can download \pp{} and extract all of the files manually. The data files should be available under \emph{test/data/}, which you can use for future alignments. 

Under \emph{test/} and \emph{examples/} you can find a variety of well-commented examples for all of the features offered by \pp{}.

\section{Features}
\label{sec:features}

\subsection{Sequence class}
\label{subsec:normseq}
The C core does not operate on sequences consist of upper-case letters of the English alphabet, but instead it uses byte arrays generated from the original sequences by deducting \emph{chr('A')} from every single character in the given sequences. Because of this, for every single operation the participant sequences should be transformed, which is clearly a waste of time.

To hide this behaviour (and cache the normalized sequences), these functions operate on a \emph{Sequence} object, which can be constructed from a string or a byte list (and their normalized versions).

\begin{lstlisting}
s1_norm = pyopa.normalize_sequence('AATCGGA')

#if the sequence comes from a normalized source
s1 = pyopa.Sequence(s1_norm, True)
s2 = pyopa.Sequence('AAAA')

print s1
print s2
#construct from a byte array, prints ACCA
print pyopa.Sequence([0, 2, 2, 0], True)

print pyopa.align_double(s1, s2, generated_env)
\end{lstlisting}


\subsection{Creating AlignmentEnvironments}
\label{subsec:createAlEnvs}
In order to operate on sequences, the distance matrix, the gap opening and extension costs and a threshold of interest must be provided. These data are stored in a so-called \alenv{} class in Python, which can be used for future alignments. 

The \alenv{} also stores different matrices and gap costs calculated from the original, provided ones. To create an environment you should provide the necessary data in a dictionary:

\begin{lstlisting}
data = {'gap_open': -20.56,
        'gap_ext': -3.37,
        'pam_distance': 150.87,
        'scores': [[10.0]],
        'column_order': 'A',
        'threshold': 50.0}

env = pyopa.create_environment(**data)

s1 = pyopa.Sequence('AAA')
s2 = pyopa.Sequence('TTT')

#prints [30.0, 2, 2, 0, 0], the first element is the score
print pyopa.align_double(s1, s1, env)

#prints [0.0, -1, -1, 0, 0], the score is 0
# since the score for 'A -> T' is undefined
print pyopa.align_double(s2, s1, env)
\end{lstlisting}

In this simple example, we used a really simple distance matrix with a single element, which means only the \emph{A $\rightarrow$ A} score is defined. When you create an \alenv{} the dimension of the matrix and the length of the columns must be the same, however, during the construction  this provided distance matrix is always extended into a 26x26 one, which consists of the scores for [A-Z]. The undefined cells of the matrix are filled with zeros (in this case every cell, except for the first one).

Because of this, aligning \emph{AAA} to \emph{AAA} gives a result of 30.0, whereas aligning the same sequence to \emph{TTT} results in zero.


Alternatively, you can either read a single \alenv{} or multiple ones from a JSON-formatted file:
\begin{lstlisting}
#loading the default environments from the data directory
# created at installation time
defaults = pyopa.load_default_environments()
env_list = defaults['environments']
log_pam1_env = defaults['log_pam1']

#the default directory (created at installation time)
matrix_dir = pyopa.matrix_dir()

#or alternatively, you can specify an exact location
env_list = pyopa.read_all_env_json(
    os.path.join(matrix_dir, 'all_matrices.json'))
log_pam1_env = pyopa.read_env_json(
    os.path.join(matrix_dir, 'logPAM1.json'))
\end{lstlisting}

Precomputed matrices stored in JSON are also coming with the package, see in Section \ref{sec:inst}.

Another possibility is to generate the distance matrices and gap costs from a single \alenv{}:

\begin{lstlisting}
#generates a signle AlignmentEnvironment
# with a pam distance of 250
generated_env = pyopa.generate_env(log_pam1_env, 250)

#generates 1000 environments for different pam distances
gen_env_list = pyopa.generate_all_env(log_pam1_env, 1000)
\end{lstlisting}

For each generation a log\_pam1 \alenv{} should be used.

\subsection{Byte and short estimations}
\label{subsec:bsEstim}
Instead of calculating the actual double score for two given sequences, we can rapidly compute an upper bound of the score. If this upper bound score is lower than the threshold of interest, we can omit the calculation of the double precision score for these particular sequences.

If the upper bound score is over the threshold, however, we have do another alignment to get the actual
 score, which can still be lower than the threshold. Since the vast majority of the scores is usually lower than the threshold, a substantial amount of time can be saved by using the byte or short estimation, because an estimation runs much faster than a double alignment.

To produce an upper bound by the short (or the byte) method, we have to transform the double distance matrix and the gap costs to shorts (or bytes). The transformation has to be done in a way that the future calculations will surely produce an upper bound score. Such a transformation could be $ceil(x)$ for every element, but this would result in a really imprecise upper bound score. Instead, we use $ceil(x \cdot factor)$ with $factor=UNSIGNED\_SHORT\_MAX / threshold$ for the short version, and a bit more complicated formula for the byte version. The factor should be designed in a way that it reduces the relative rounding errors, for a more precise estimation.

After the transformations, we can cram multiple elements into a single register (8 for the short version and 16 for the byte version, assuming 128-bit SSE registers) and perform the calculation. Of course the result score has to be scaled back by the used $factor$.

To hasten the estimation further, we can create so-called profiles for a given $S$ sequence and matrix combination. After a profile is created, we can use it for multiple alignments to the same $S$ and matrix combination (the gap scores can be changed, by redefining in the provided \alenv{}):

\begin{lstlisting}
s1 = pyopa.Sequence('AATCGGA')
s2 = pyopa.Sequence('AAAA')
s3 = pyopa.Sequence('CATACCTGGTGTGATGCC')

#not optimal, multiple hidden profile generations in the background
print pyopa.align_short(s1, s2, generated_env)
print pyopa.align_short(s1, s3, generated_env)
print pyopa.align_byte(s1, s2, generated_env)
print pyopa.align_byte(s1, s3, generated_env)

#one profile generation
profile = pyopa.AlignmentProfile()
profile.create_profiles(s1, generated_env)

#the following code produces the exact same result
#but is more efficient since it's
#using the same profile for multiple alignments
print profile.align_short(s2, generated_env)
print profile.align_short(s3, generated_env)
print profile.align_byte(s2, generated_env)
print profile.align_byte(s3, generated_env)
\end{lstlisting}

The scaled short and byte matrices, which are used for profile generation, and the gap costs are automatically computed during the creation of an \alenv{}. However, if you change the threshold, the element of the original matrix or the gap costs the short and byte ones have to be recomputed:
\begin{lstlisting}
generated_env.threshold = 50.0
#WRONG: the short and byte matrices are not recomputed!!!
# the matrices and gap costs for the old threshold
# will be used
profile.create_profiles(s1, generated_env)

#recompute the byte/short matrices and gap costs
generated_env.create_scaled_matrices()

#and then create the profile
profile.create_profiles(s1, generated_env)

#now we can do alignments with the new profile:
print profile.align_short(s3, generated_env)
\end{lstlisting}

\subsection{Computing the actual score}
\label{subsec:compAct}

Whenever the estimation is over the threshold, we have to compute the actual score, which can still be below the threshold. In \pp{} there are two possible ways to do so, however, only one of them is an efficient vectorised approach.

The first available Python function is a reference implementation of the Smith-Waterman algorithm, and should run on every system without problems. It does not use any vectorisation, therefore it is clearly not an efficient solution. This reference implementation is only capable of local alignments and provides a single double precision score as a result but not the ranges of the local alignment:

\begin{lstlisting}
#always a local alignment
print pyopa.align_scalar_reference_local(s1, s2, generated_env)
\end{lstlisting}

The second, more efficient and vectorised solution is also capable of global alignments and can also provide the ranges of the local alignment on demand. By setting the \emph{stop\_at\_threshold} argument true, we can terminate the score computation if it reaches the provided threshold:

\begin{lstlisting}
s1 = pyopa.Sequence('AATCGGA')
s3 = pyopa.Sequence('CATACCTGGTGTGATGCC')
#does not stop at threshold,
#  it is NOT a global alignment, and computes the ranges
#returns [19.946, 6, 8, 0, 2], the first element is the score
# the 4 other elements are [max1, max2, min1, min2] the ranges
print pyopa.align_double(s1, s3, generated_env, False, False, True)

#returns [score, max1, max2] because the last flag (calculate ranges) is false
print pyopa.align_double(s1, s3, generated_env, False, False, False)

generated_env.threshold = 10.0
#no generated_env.create_scaled_matrices() is needed
# because we do not operate on short/byte matrices

#results in [11.499268729503227, 3, 0, 3, 0], not the best possible
# local alignment, but still over the threshold of 10.0
print pyopa.align_double(s1, s3, generated_env, True, False, True)

#global alignment, stop at threshold is ignored
print pyopa.align_double(s1, s3, generated_env, True, True, True)
\end{lstlisting}

In the example given above, \emph{s1[0:6] = AATCGGA} has been aligned to \emph{s3[2:8] = TACCTGG}. If we do not require the full ranges, the \emph{max1} and \emph{max2} values are still provided, so in this case the result is an array with three elements.

\subsection{Getting the concrete alignment}
\label{subsec:concrete}

In some cases we might be interested not only in the score, but also in a concrete alignment of the two sequences. With \pp{} this can be computed by using a single function, however, this function requires a huge amount of stack to work correctly. On UNIX-based systems you can unlimit your stack size by using the following Python code snippet:
\begin{lstlisting}
import resource

resource.setrlimit(resource.RLIMIT_STACK, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))
\end{lstlisting}

Another, more general approach is to start the operation in a new thread with a given stack size:

\textbf{Note that since pyopa version 0.8 this problem has been resolved. pyopa does no longer need a large stack.}

\begin{lstlisting}
#to do the concrete alignment in a new thread
#or alternatively you can increase your stack size on UNIX-based systems:
#'resource.setrlimit(resource.RLIMIT_STACK, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))'
def nt_align(s1, s2, env, is_global, aligned_strs):
    print 'Concrete %s alignment:' % ('global' if is_global else 'local')
    tmp_aligned_strings = pyopa.align_strings(s1, s2, env, is_global)
    print '\taligned_s1: %s' % tmp_aligned_strings[0]
    print '\taligned_s2: %s' % tmp_aligned_strings[1]
    aligned_strs.extend(tmp_aligned_strings)

s1 = pyopa.Sequence('PISRIDNNKITTTLGNTGIISVTIGVIIFKDLHAKVHGF')
s2 = pyopa.Sequence('PIERIENNKILANTGVISVTIGVIIYQDLHADTVMTSDY')
#threading.stack_size(100000000)

# aligned_s1: PISRIDNNKITTTLGNTGIISVTIGVIIFKDLHAKV
# aligned_s2: PIERIENNKI___LANTGVISVTIGVIIYQDLHADT
aligned_strings = []
t = threading.Thread(None, nt_align,
                     'Aligning Thread', (s1, s2, generated_env, False, aligned_strings))
t.start()
t.join()
print aligned_strings[0]
print aligned_strings[1]
\end{lstlisting}

\subsection{EstimatePam}
\label{subsec:EPam}

For two given aligned sequences, the \emph{EstimatePam} function computes maximum likelihood estimates on the score and pam distance, and the variance. 

From a list of \emph{AlignmentEnvironments} and a \emph{log\_pam1} environment we can create the necessary data structure which can be used for computing the estimations.

\begin{lstlisting}
dms = pyopa.MutipleAlEnv(gen_env_list, log_pam1_env)

#returns an array: [similarity, pam_distance, variance]
print dms.estimate_pam(aligned_strings[0], aligned_strings[1])

\end{lstlisting}

The first element of the array is the estimated similarity score, which is always higher than the actual score, the second element is the estimated evolutionary distance and the third one is the variance.

The input strings (\emph{s1} and \emph{s2}) must have the same length, since they have to be a concrete alignment pair produced by \emph{align\_strings} (see in Section \ref{subsec:concrete}).

\section{Comparison with PyCogent}
\label{sec:compCogent}

\section{Conclusion}
\label{sec:concl}

With its core written in C, \pp{} offers and efficient vectorized implementation of the Smith-Waterman algorithm as well as many other features including byte and short estimations and computing the concrete strings.

All of the operations can be performed with the convenience of Python, in a platform independent way, which makes it an enticing choice for applications operating on immense amount of sequences, where efficiency is of paramount importance.

\end{document}
